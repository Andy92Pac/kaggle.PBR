---
output:
  html_document: default
  pdf_document: default
stitle: Rapport
---
## Plan

1. Introduction
2. Présentation des données
3. Méthodologie
4. Modèles
    + Simples
    + Ensembles de modèles
        + Boosting
        + Stacking
5. Evaluation
6. Conclusion
7. Annexes

***

> MetaData

Chargement des librairies utile à ce projet

```{r LibraryCheck, message=FALSE, warning=FALSE}
library(caret) #Selection de variable NearZeroVar()
library(caTools)
library(FactoMineR) #ACP
library(xgboost) #Xgboost
library(e1071) #SVM
```

***

> Introduction

Aujourd’hui, les volumes de données récoltées sont de plus en plus grands et ces quantités ne font qu’augmenter. Cela a pour conséquence de permettre de réaliser de nombreuses analyses sur ces données et d’en tirer de multiples informations, jusqu’à pouvoir prédire avec une certaine confiance des événements. 

De nombreux projets pivotent autour de cette problématique de prédictions d'événements. Des sociétés en viennent même à soumettre des projets sous forme de compétition sur des sites tels que Kaggle.

Nous avons réalisé un de ces projets qui a été soumis par la société Boehringer Ingelheim. Ce projet consiste à prédire à quelle classe biologique appartiendrait une molécule en fonction de ses propriétés chimiques.
Nous avons pour cela le langage R, très adapté aux études statistiques et aux problèmes de Data Science.

Ce rapport a pour but d’expliquer en plusieurs axes notre démarche dans la construction d’un modèle de prédiction répondant à ce problème.

Pour ce fait nous allons en premier lieu présenter les données; la méthodologie utilisée , les différents modèles que nous avons abordés et celui qu’on a retenu ; les résultats obtenus et enfin une conclusion.

De plus nous rendront le code disponible en annexe.

***

> Présentation des données

Dans ce genre de problème, la maîtrise des données utilisées est souvent fondamentale.
Nous allons donc dans un premier temps étudier ces dernières afin d’orienter correctement la suite de notre travail. 
Le jeu de données fourni est un fichier au format csv. Nous chargeons donc ce dernier via la commande read.csv.

```{r}
#Chargement du csv train.csv
train.data <- read.csv('data/train.csv')
```

Une fois les données accessibles dans notre environnement de travail, nous appliquons plusieurs commandes nous permettant d’en apprendre plus sur celles-ci.
Nous avons donc à notre disposition un dataset composé de 3751 individus décrits chacun par un total de 1777 variables.

```{r}
#Nombres de variables et d'individus
dim(train.data)
```

Les individus sont ici des molécules. Parmi les 1777 variables décrivant chaque molécule, la première, nommée “Activity” correspond à la réponse biologique de celle ci. Les 1776 autres variables donnent elles des informations concernant les propriétés chimiques des molécules.

```{r}
#6 premières observations sur les 10 premières variables
head(train.data[,1:10],n = 6)
```

Ces variables sont toutes de type numérique. De plus, on constate rapidement que la totalité d’entre elles prennent valeur dans l’intervalle [0,1]. Les données sont donc normalisées. Certaines de ces variables sont de type binaire et ne prennent donc comme valeur que 0 et 1. D’autres sont elles de type quantitatif discrète. Enfin, on constate également la présence de variables quantitatives continues.

```{r}
#Description du type des 10 premières variables
str(train.data[,1:10])
```

***

> Methodologie

Après la première phase consistant à en apprendre plus sur les données utilisées, nous pouvons passer à l’étape suivante.
Nous savons désormais que le jeu de données contient 1776 variables explicatives. Le nombre étant relativement élevé, on peut imaginer que l’ensemble de celles-ci n’ont pas la même importance dans l’obtention du résultat de la réaction biologique de la molécule.

Il apparaît donc utile de réduire le nombre de dimensions si certaines d’entre elles ne sont pas pertinentes dans la création de notre modèle prédictif. Cependant, il n’est pas certain que les méthodes de réduction de dimensions améliorent nos modèles. Nous réaliserons donc à chaque fois un modèle sur les données réduite mais également sur le dataset entier afin de vérifier si cette réduction améliore les prédictions ou non.

L’évaluation du modèle prédictif est l’une des étapes les plus importantes. Il existe de nombreuses métriques permettant cette évaluation et les résultats retournés différents selon le choix réalisé. Il est donc primordial de choisir un mesure d’évaluation adaptée au problème que l’on tente de résoudre. Dans notre cas, la métrique utilisée pour évaluer la qualité des prédictions soumise sur Kaggle est le log-loss.

Nous utiliserons donc cette métrique afin d’évaluer la qualité de nos modèles sur les données du dataset de validation

```{r echo=TRUE}
#Fonction de calcul du logloss
LogLossBinary = function(actual, predicted, eps = 1e-15) {  
  predicted = pmin(pmax(predicted, eps), 1-eps)  
  - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
}
```

#####__Sélection de variables__

Sélection de varibale grâce la fonction nearZeroVar() du package caret qui permet d'eliminer les variable avec une information peu pertinente du fait que leurs variances tend vers 0 

```{r}
#Supression des variable dont la variance est ègale à 0 ou tend vers 0
remove.cols <- nearZeroVar(train.data[,-1], names = T)

#Ensemble des colonnes du jeu de données Train.csv
all.cols <- names(train.data)

#Ensemble de données - données supprimé = nouveau jeu de données de 799 variables 
#Soit une réduction de 978 variables
train.reduc <- train.data[,setdiff(all.cols, remove.cols)]

#Pour la suite du code on met les informations de train.reduc dans train
train <- train.reduc

```

#####__Spliting de données__

Une fois la sélection faite , on split notre jeu de données en 2 parties : 
 - Training qui contiens 85% des données provenant de train
 - Validation qui contiens 15% des données provenant de train
 
Il est important à cette étape de séparer notre jeu de données de manière à avoir deux datasets conservant le maximum l'information initiale contenue dans le dataset complet.
 
On se basera sur le jeu de données training pour créer et entrainer notre modèle 
et nous valideront notre modèle sur le jeu de données validation.

```{r echo=TRUE}

#Variable expliquée : Activity
outcomeName <- names(train)[1]

#Variable explicatives Ensemble du jeu de données de 799 - Activity
predictorNames <- setdiff(names(train), outcomeName)

#Spliting de notre jeu de données en trainning et validation
set.seed(12345)
train$spl = sample.split(train[,1], SplitRatio = 0.85)
training = train[train$spl==1,]
validation = train[train$spl==0,]

#On retire aux jeu de données training et validation les variables spl
training <- subset(training, select = -c(spl))
validation <- subset(validation, select = -c(spl))

```

Conversion du type integer en factor() pour que les modèles simple puissent traiter le problème en classification de deux classes.

```{r echo=FALSE, results='hide',message=FALSE}
#Conversion integer vers factor()
training[,outcomeName] <- ifelse(training[,outcomeName]==1,'yes','nope')
```


#####__*REM:*__

*les méthodes de selection de variables ainsi que le fait de convertir en factor() notre variable à prédire sont utilisé pour optimiserles méthodes simples SVM et Random Forest.*

La méthodologie utilisée concernant la création de nos modèles prédictifs est détaillée dans la suite de ce document.

***

> Modèles simples

SVM : explication rapide de ce que c'est + explication des paramètres

```{r eval=FALSE}
svm.model <- svm(training[,outcomeName]~. , #Formule Activity en fonction de toutes les autres variables de training
                 data = training[,predictorNames],
                 type="C-classification", #Classe 0 et 1
                 probability = TRUE  #Pour avoir des probabilités 
                 )
```

Random Forest : 

Le principe est d'utiliser un grand nombre d'arbres de décision construits chacun avec un sous-échantillon différent de l'ensemble d'apprentissage, et pour chaque construction d'arbre, la décision à un noeud est fait en fonction d'un sous-ensemble de variables tirées au hasard.

puis, on utilise l'ensemble des arbres de décision produits pour faire la prédiction, avec un vote à la majorité (pour de la classification, variable prédite de type facteur), ou une moyenne (pour de la régression, variable prédite de type numérique).

```{r eval=FALSE}
#Construction du modèle Random Forest
rf.modelProba <- randomForest(x = training[,predictorNames], #Variables explicative
                              y = training[,outcomeName], #Variable expliquée
                              ntree = 100, #Nombres d'arbres donc d'itérations
                              do.trace = TRUE) #Identique à verbose pour avoir plus d'information sur l'éxecution

#Prediction du random forest sur validation
valid.predProbaRF <- predict(rf.modelProba,validation[,predictorNames],type = "prob")
```


***

> Ensemble de modèle

Lorsqu’il faut prendre une décision importante, il vaut souvent mieux recueillir plusieurs avis que de se fier à un seul. Utiliser un modèle de machine learning pour prédire un comportement ou un prix, c’est un premier pas. 

Mais agréger des milliers de modèles ayant des avis divergents mais pouvant être chacun spécialisés sur des parties de la data donne le plus souvent de meilleurs résultats. Nous parlons alors de méthodes ensemblistes, dont les plus connues sont le bagging, boosting et stacking.
 
Nous allons aborder ici deux de ces méthodes le boosting et le stacking.

> Boosting

Le principe du boosting est quelque peu différent du bagging. Les différents classifieurs sont pondérés de manière à ce qu’à chaque prédiction, les classifieurs ayant prédit correctement auront un poids plus fort que ceux dont la prédiction est incorrecte.
 
XGboost est un algorithme de boosting qui s’appuie sur ce principe, avec un paramètre de mise à jour adaptatif permettant de donner plus d’importance aux valeurs difficiles à prédire, donc en boostant les classifieurs qui réussissent quand d’autres ont échoué. 

Des variantes permettent de l’étendre à la classification multiclasses. Adaboost s’appuie sur des classifieurs existants et cherche à leur affecter les bons poids vis à vis de leurs performances.

Après les premiers résultats encourageants obtenus avec les premieres méthodes utilisées, nous avons décidé d'utiliser le boosting afin de voir si cela nous permettait d'obtenir de meilleurs résultats.
Le package xgboost qui met à disposition des méthodes de ce type a donc été choisi pour cela.
Nous avons dans un premier temps appliqué l'algorithme sur notre jeu de données de training en choisissant comme mesure d'erreur le LogLoss.

```{r}
#Données training mis dans une matrix pour le traitement xgb
training <- data.matrix(training)

#Création du modèle xgboost sans optimisation
xgb.model <- xgboost(data = training[,predictorNames], label = training[,outcomeName], nrounds = 2)
```

La fonction nous retourne à chaque itération la nouvelle valeur de l'erreur calculée. Comme nous pouvons le constater, cette erreur diminue au fur et à mesure des itérations. Le problème est qu'une fois un certain seuil passé, notre algorithme fait du surapprentissage. Le modèle apprend donc "par coeur" les données utilisées et n'est plus capable de généraliser le problème qu'il essaye de prédire. 

Afin de palier à cela, il faudrait pouvoir suivre l'évolution du taux d'erreur lorsque le modèle est appliqué sur des données qui ne sont pas utilisées pour sa génération. En effet, lorsque le modèle commencera à faire du surapprentissage, on constatera que l'erreur calculée sur les prédictions des données de validation ne descendra plus mais augmentera.
Nous avons donc utilisé, cette pratique, appelée la validation croisée (cross validation).

```{r echo=FALSE}
train <- train.data
#Variable expliquée : Activity
outcomeName <- names(train)[1]

#Variable explicatives Ensemble du jeu de données de 799 - Activity
predictorNames <- setdiff(names(train), outcomeName)

#Spliting de notre jeu de données en trainning et validation
set.seed(12345)
train$spl = sample.split(train[,1], SplitRatio = 0.85)
training = train[train$spl==1,]
validation = train[train$spl==0,]

#On retire aux jeu de données training et validation les variables spl
training <- subset(training, select = -c(spl))
validation <- subset(validation, select = -c(spl))

training <- data.matrix(training)

```

```{r, R.options=list(max.print=10), results='hide'}
#Données validation mis dans une matrix pour le traitement xgb
validation <- data.matrix(validation)

#Création des matrix xgb
training.Dmat <- xgb.DMatrix(training[,predictorNames], label=training[,outcomeName])
valid.Dmat <- xgb.DMatrix(validation[,predictorNames], label=validation[,outcomeName])

#Utilisation des matrix crée dans la watchlist qui va nous permettre de créer un modele pour chaque matrix donnée en paramètres
watchlist <- list(train = training.Dmat, valid =  valid.Dmat)

#Construction du modèle xgb
cv.data <- xgb.train(data = training.Dmat
                     ,nrounds = 100
                     ,eta = 0.2
                     ,max_depth = 6
                     ,watchlist = watchlist
                     ,eval_metric = "logloss"
                     ,objective = "binary:logistic")

```

```{r, echo=FALSE}
head(cv.data$evaluation_log)
tail(cv.data$evaluation_log)
```

Il ne nous reste donc plus qu'à identifier à quel moment le taux d'erreur des données de validation est-il minimisé pour obtenir le nombre d'itération permettant d'optimiser le plus possible notre modèle sans tomber toutefois dans le surapprentissage.

```{r}
#Récuperation du logloss le moins élevé de validation
min.logloss <- min(cv.data$evaluation_log$valid_logloss)
min.logloss

#Récuperation du nombre de fois qu'il à du tourner pour avoir le logloss optimisé
min.logloss.index <- which.min(cv.data$evaluation_log$valid_logloss)
min.logloss.index
```

Ce graphique permet de constater visuellement le nombre d'itération à partir duquel le surapprentissage survient et on constate en effet qu'à partir d'un certain moment le taux d'erreur du dataset de validation ne diminue plus mais au contraire augmente.

```{r, echo=FALSE}
plot(cv.data$evaluation_log$train_logloss, type = 'l', col="blue", lwd = 2)
lines(cv.data$evaluation_log$valid_logloss, col = 'red', lwd = 2)
segments(min.logloss.index, 0, y1 = min.logloss, lty = 'dotted')
segments(0, min.logloss, min.logloss.index, min.logloss, lty='dotted')
```

Nous avons ensuite voulu améliorer notre modèle en jouant avec les hyperparamètres de l'algorithme. Après avoir tenté d'ajuster ces derniers à la main, nous avons fait le choix d'utiliser une approche plus rigoureuse en utlisant un grid search afin d'identifier les hyperparamètres optimaux pour notre modèle. Nous avons donc choisi 4 paramètres que nous avons jugé succeptible d'améliorer notre modèle :
- eta
- gamma
- max_depth
- max_delta_step

```{r}
gs <- list(eta = c(0.1,0.3,0.5)
           ,gamma = c(0, 0.1, 0.5)
           ,max_depth = c(3, 6, 10)
           ,max_delta_step = c(0, 2)
)
gs.df <- expand.grid(gs)
gs.df <- cbind(gs.df, 1:nrow(gs.df))
```

Une fois que nous avons fini de constituer notre matrice contenant les différentes valeurs choisies pour les hyperparamètres, nous appliquons l'algorithme sur chacune des lignes de cette dernière.

```{r eval=FALSE}
res.gs <- apply(gs.df, 1, function(row) {
  cv.data <- 0
  cv.data <- xgb.train(data = training.Dmat
                       ,nrounds = 100
                       ,eta = row[1]
                       ,gamma = row[2]
                       ,max_depth = row[3]
                       ,max_delta_step = row[4]
                       ,watchlist = watchlist
                       ,eval_metric = "logloss"
                       ,objective = "binary:logistic")
  min.val <- min(cv.data$evaluation_log$valid_logloss)
  min.index <- which.min(cv.data$evaluation_log$valid_logloss)
  c(min.val, min.index)
})

res.gs.t <- t(res.gs)
gs.df <- cbind(min.val = gs.df, min.index = res.gs.t)
```

Les valeurs minimales obtenues pour chaque combinaison d'hyperparamètres sont ajoutées à la matrice initiale et il ne nous reste donc plus qu'à chercher la ligne pour laquelle cette valeur est la plus faible pour savoir quelle combinaison est optimale.

```{r}
min.logloss.gs <- min(gs.df$min.val)
min.logloss.gs
min.logloss.gs.index <- which.min(gs.df$min.val)
min.logloss.gs.index
min.logloss.gs.nrounds <- gs.df$min.index[10]
min.logloss.gs.nrounds
```



> Stacking

Le stacking (ou dit parfois blending) est un procédé qui consiste à appliquer un algorithme de machine learning à des classifieur générés par un autre algorithme de machine learning.
 
D’une certaine façon, il s’agit de prédire quels sont les meilleurs classifieurs et de les pondérer. Cette démarche a l’avantage de pouvoir agréger des modèles très différents et d’améliorer sensiblement la qualité de la prédiction finale, le challenge NetFlix à 1 million $ en est la meilleure preuve.
Pour pouvoir évaluer les différents modèles à stacker, on utilise des poids constants permettant de pondérer les différents modèles. C’est du stacking linéaire standard.
 
La principale limitation est qu’on a perdu la dépendance des modèles à leur data et plus particulièrement leur spécialisation. On va donc utiliser des poids variables qui dépendent des données ; soit des méta-features. Le stacking est cette fois-ci dépendant du poids des features.

Svm Model : 

```{r echo=FALSE, results='hide',message=FALSE}
#Conversion integer vers factor()
training[,outcomeName] <- ifelse(training[,outcomeName]==1,'yes','nope')
```

```{r eval=FALSE}
#Creation du modèle SVM
svm.model <- svm(training[,outcomeName]~. ,
                 data = training[,predictorNames],
                 type="C-classification",
                 probability = TRUE
                 )

pred.svm <- predict(svm.model, test, probability = TRUE)
```

XGB Model : 

```{r eval=FALSE}
xgb.model <- xgb.train(data = training.Dmat
                       ,nrounds = opti.nrounds
                       ,eta = 0.2
                       ,max_depth = 6
                       ,eval_metric = "logloss"
                       ,objective = "binary:logistic")

pred.xgb <- predict(xgb.model, as.matrix(test))
```

Attribution des poids et Ensembling : 

```{r eval=FALSE}
w.xgb = 5
w.svm = 3
pred.ens <- (pred.xgb*w.xgb+pred.svm[,1]*w.svm)/(w.xgb+w.svm)
```

***

> Evaluation

Résulats obtenues de notre meilleure modèle


***

> Conclusion

conclusion

